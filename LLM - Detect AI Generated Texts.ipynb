{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tensorflow.keras import regularizers\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.data.path.append('/kaggle/input')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Setting a seed for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    return [lemma.name() for syn in nltk.corpus.wordnet.synsets(word) for lemma in syn.lemmas()]\n",
    "\n",
    "def synonym_replacement(words, n=1):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in nltk.corpus.stopwords.words('english')]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "def random_deletion(words, p=0.5):\n",
    "    if len(words) == 1: \n",
    "        return words\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p, words)) \n",
    "    if len(remaining) == 0: \n",
    "        return [random.choice(words)] \n",
    "    else:\n",
    "        return remaining\n",
    "\n",
    "def random_swap(sentence, n=1): \n",
    "    length = range(len(sentence)) \n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(length, 2)\n",
    "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
    "    return sentence\n",
    "\n",
    "def broken_words(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    for _ in range(n):\n",
    "        word = random.choice(words)\n",
    "        if len(word) > 1:\n",
    "            pos = random.randint(0, len(word)-1)\n",
    "            new_word = word[:pos] + word[pos+1:]\n",
    "            words = [new_word if w == word else w for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def add_typos(sentence, n=1):\n",
    "    typos = []\n",
    "    for _ in range(n):\n",
    "        typo_pos = random.randint(0, len(sentence)-1)\n",
    "        if random.random() < 0.5:  # deletion\n",
    "            typo = sentence[:typo_pos] + sentence[typo_pos+1:]\n",
    "        else:  # replacement\n",
    "            typo = sentence[:typo_pos] + random.choice(string.ascii_lowercase) + sentence[typo_pos+1:]\n",
    "        typos.append(typo)\n",
    "    return typos\n",
    "\n",
    "def add_repetition(sentence, n=2):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "    sentence += ' ' + ' '.join(random.choices(words, k=n))\n",
    "    return sentence\n",
    "\n",
    "def add_semantic_drift(sentences):\n",
    "    random.shuffle(sentences)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "#loading data\n",
    "train_essays = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n",
    "test_essays = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "\n",
    "# Preprocessing data\n",
    "train_texts = train_essays['text'].tolist()\n",
    "train_labels = train_essays['generated'].tolist()\n",
    "test_texts = test_essays['text'].tolist()\n",
    "\n",
    "# Calculate the number of entries per augmentation function\n",
    "entries_per_function = 250\n",
    "\n",
    "# Apply each augmentation function and add the augmented texts and labels to the train_texts and train_labels lists\n",
    "for i in range(entries_per_function):\n",
    "    index = random.randint(0, len(train_texts) - 1)\n",
    "    text = train_texts[index]\n",
    "    label = train_labels[index]\n",
    "\n",
    "    augmented_text = synonym_replacement(text.split(), n=5)\n",
    "    augmented_label = random.uniform(0.6, 0.9)\n",
    "    train_texts.append(augmented_text)\n",
    "    train_labels.append(augmented_label)\n",
    "\n",
    "    augmented_text = ' '.join(random_swap(text.split(), n=5))\n",
    "    augmented_label = random.uniform(0.2, 0.4)\n",
    "    train_texts.append(augmented_text)\n",
    "    train_labels.append(augmented_label)\n",
    "    \n",
    "    augmented_text = ' '.join(random_deletion(text.split(), p=0.5))\n",
    "    augmented_label = random.uniform(0.2, 0.5)\n",
    "    train_texts.append(augmented_text)\n",
    "    train_labels.append(augmented_label)\n",
    "\n",
    "    augmented_text = broken_words(text, n=5)\n",
    "    augmented_label = random.uniform(0.3, 0.5)\n",
    "    train_texts.append(augmented_text)\n",
    "    train_labels.append(augmented_label)\n",
    "\n",
    "    typo_texts = add_typos(text, n=5)\n",
    "    for typo_text in typo_texts:\n",
    "        augmented_label = random.uniform(0.2, 0.5)\n",
    "        train_texts.append(typo_text)\n",
    "        train_labels.append(augmented_label)\n",
    "    \n",
    "    augmented_text = add_repetition(text, n=2)\n",
    "    augmented_label = random.uniform(0.6, 0.9)\n",
    "    train_texts.append(augmented_text)\n",
    "    train_labels.append(augmented_label)\n",
    "\n",
    "# For add_semantic_drift, we can just add one entry\n",
    "augmented_text = add_semantic_drift(train_texts)\n",
    "augmented_label = random.uniform(0.6, 0.9)\n",
    "train_texts.append(augmented_text)\n",
    "train_labels.append(augmented_label)\n",
    "\n",
    "# Now, we randomly select 2700 entries from train_texts and train_labels\n",
    "indices = list(range(len(train_texts)))\n",
    "random.shuffle(indices)\n",
    "selected_indices = indices[:2700]\n",
    "train_texts = [train_texts[i] for i in selected_indices]\n",
    "train_labels = [train_labels[i] for i in selected_indices]\n",
    "\n",
    "# Tokenizing data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased-model/bert-base-uncased\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Converting data into TensorFlow format\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
    "    train_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},\n",
    "))\n",
    "\n",
    "# Definining model components\n",
    "bert_layer = TFAutoModel.from_pretrained(\"/kaggle/input/bert-base-uncased-model/bert-base-uncased\", from_pt=True)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.1)  \n",
    "output_layer = tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "# Defining model\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "outputs = bert_layer([input_ids, attention_mask])\n",
    "dropout_outputs = dropout_layer(outputs[1])\n",
    "output_layer_outputs = output_layer(dropout_outputs)\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output_layer_outputs)\n",
    "\n",
    "# Compiling model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training model\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=2)\n",
    "\n",
    "# Saving the model\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Predicting test data\n",
    "predictions = model.predict(test_dataset.batch(16))\n",
    "\n",
    "output = pd.DataFrame({'id': test_essays.id, 'generated': predictions[:, 1]})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission was successfully saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
